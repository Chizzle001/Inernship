{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5b5f7e",
   "metadata": {},
   "source": [
    "## ASSIGNMENT - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961f914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTNG NECESSARY LIBRARIES \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b381d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad292f2e",
   "metadata": {},
   "source": [
    "#### Q.1 Write a python program to display all the header tags from wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "064b8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Main_Page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a148ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wikipedia_headers(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # We Need to scrap all the header tags so lets create an empty list\n",
    "    \n",
    "    Headers = [i.text for i in soup.find_all('span',class_='mw-headline')]\n",
    "    \n",
    "    # Using List comprehension to scrape the data\n",
    "            \n",
    "    print(Headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ce53152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome to Wikipedia', \"From today's featured article\", 'Did you know\\xa0...', 'In the news', 'On this day', \"From today's featured list\", \"Today's featured picture\", 'Other areas of Wikipedia', \"Wikipedia's sister projects\", 'Wikipedia languages']\n"
     ]
    }
   ],
   "source": [
    "Wikipedia_headers(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f192c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1471f735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2800a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "453df474",
   "metadata": {},
   "source": [
    "#### Q2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18595fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.imdb.com/search/title/?count=100&groups=top_1000&sort=user_rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56a229ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IMDB_top_movies(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # Since we Need information on Name , year and Rating, we create a empty List for each feature\n",
    "    \n",
    "    Movie_name = [name.a.text.replace('\\n','') for name in soup.find_all('h3',class_='lister-item-header')]\n",
    "    Year_of_Release = [year.text.replace('(','').replace(')','') for year in soup.find_all('span',class_='lister-item-year text-muted unbold')]\n",
    "    Rating = [rate.text.replace('\\n','') for rate in soup.find_all('div',class_='inline-block ratings-imdb-rating')]\n",
    "    \n",
    "    # We Create a DataFrame \n",
    "    data = {'Name of move': Movie_name, 'Year of Release': Year_of_Release, 'Rating': Rating}\n",
    "    Movie_df = pd.DataFrame(data, index=range(1,101))\n",
    "    print(Movie_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675bf4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Name of move Year of Release Rating\n",
      "1                         The Shawshank Redemption            1994    9.3\n",
      "2                                    The Godfather            1972    9.2\n",
      "3                                  The Dark Knight            2008    9.0\n",
      "4    The Lord of the Rings: The Return of the King            2003    9.0\n",
      "5                                 Schindler's List            1993    9.0\n",
      "..                                             ...             ...    ...\n",
      "96                     Witness for the Prosecution            1957    8.4\n",
      "97                                  Paths of Glory            1957    8.4\n",
      "98                                    Sunset Blvd.            1950    8.4\n",
      "99                              The Great Dictator            1940    8.4\n",
      "100                                     Chhichhore            2019    8.3\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "IMDB_top_movies(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc843ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2a090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c20c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27d0c97c",
   "metadata": {},
   "source": [
    "#### Q.3 Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "216bc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.imdb.com/india/top-rated-indian-movies/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "461d7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Indian_Movies(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # Since we Need information on Name , year and Rating, we create a empty List for each feature\n",
    "    \n",
    "    Indian_Movie_name = [name.a.text for name in soup.find_all('td',class_='titleColumn')][:99]\n",
    "    \n",
    "    Ratings = [rate.text.replace('\\n','') for rate in soup.find_all('td',class_='ratingColumn imdbRating')][:99]\n",
    "    \n",
    "    Released_Year = [year.text.replace('(','').replace(')','') for year in soup.find_all('span',class_='secondaryInfo') ][:99]\n",
    "    \n",
    "    # Creating and putting values in a DataFrame\n",
    "    data = {'Name of movie': Indian_Movie_name, 'Year of Release': Released_Year, 'Rating': Ratings}\n",
    "    \n",
    "    Indian_Movie_df = pd.DataFrame(data, index=range(1,100))\n",
    "    print(Indian_Movie_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a94ab37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Name of movie Year of Release Rating\n",
      "1   Ramayana: The Legend of Prince Rama            1993    8.5\n",
      "2            Rocketry: The Nambi Effect            2022    8.4\n",
      "3                               Golmaal            1979    8.4\n",
      "4                           777 Charlie            2022    8.4\n",
      "5                               Nayakan            1987    8.4\n",
      "..                                  ...             ...    ...\n",
      "95           The Legend of Bhagat Singh            2002    8.0\n",
      "96                       Kaakkaa Muttai            2014    8.0\n",
      "97                          Ustad Hotel            2012    8.0\n",
      "98             Theeran Adhigaaram Ondru            2017    8.0\n",
      "99                      Rang De Basanti            2006    8.0\n",
      "\n",
      "[99 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "Indian_Movies(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71550b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172483d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a79e73c5",
   "metadata": {},
   "source": [
    "#### Q.4 Write  python program to display list of respected former presidents of India(i.e. Name , Term of office)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af84af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6137cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Presidents(url):\n",
    "    \n",
    "    # Taking request from Website to perform scraping\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # Extarcting the data for Name and Term of Office \n",
    "    \n",
    "    President_name = [name.h3.text for name in soup.find_all('div',class_='presidentListing')]\n",
    "    Term_of_Office = [term.p.text.replace('Term of Office: ','') for term in soup.find_all('div',class_='presidentListing')]\n",
    "      \n",
    "    # Converting Above Data into DataFrame\n",
    "    \n",
    "    data = {'Name':President_name,'Term of Office':Term_of_Office}\n",
    "    Former_Presidents_df = pd.DataFrame(data)\n",
    "    \n",
    "    print(Former_Presidents_df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba20618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Name  \\\n",
      "0           Shri Ram Nath Kovind (birth - 1945)   \n",
      "1             Shri Pranab Mukherjee (1935-2020)   \n",
      "2   Smt Pratibha Devisingh Patil (birth - 1934)   \n",
      "3            DR. A.P.J. Abdul Kalam (1931-2015)   \n",
      "4            Shri K. R. Narayanan (1920 - 2005)   \n",
      "5           Dr Shankar Dayal Sharma (1918-1999)   \n",
      "6               Shri R Venkataraman (1910-2009)   \n",
      "7                  Giani Zail Singh (1916-1994)   \n",
      "8         Shri Neelam Sanjiva Reddy (1913-1996)   \n",
      "9          Dr. Fakhruddin Ali Ahmed (1905-1977)   \n",
      "10     Shri Varahagiri Venkata Giri (1894-1980)   \n",
      "11                 Dr. Zakir Husain (1897-1969)   \n",
      "12     Dr. Sarvepalli Radhakrishnan (1888-1975)   \n",
      "13             Dr. Rajendra Prasad (1884-1963)    \n",
      "\n",
      "                                       Term of Office  \n",
      "0                     25 July, 2017 to 25 July, 2022   \n",
      "1                     25 July, 2012 to 25 July, 2017   \n",
      "2                     25 July, 2007 to 25 July, 2012   \n",
      "3                     25 July, 2002 to 25 July, 2007   \n",
      "4                     25 July, 1997 to 25 July, 2002   \n",
      "5                     25 July, 1992 to 25 July, 1997   \n",
      "6                     25 July, 1987 to 25 July, 1992   \n",
      "7                     25 July, 1982 to 25 July, 1987   \n",
      "8                     25 July, 1977 to 25 July, 1982   \n",
      "9                24 August, 1974 to 11 February, 1977  \n",
      "10  3 May, 1969 to 20 July, 1969 and 24 August, 19...  \n",
      "11                        13 May, 1967 to 3 May, 1969  \n",
      "12                       13 May, 1962 to 13 May, 1967  \n",
      "13                   26 January, 1950 to 13 May, 1962  \n"
     ]
    }
   ],
   "source": [
    "Presidents(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7dc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536aad72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8de56d9",
   "metadata": {},
   "source": [
    "#### Q5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "    a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ff408fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52509210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_teams_mens(url):\n",
    "     \n",
    "    # Taking request from Website to perform scraping\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # We create a empty list for all attributes\n",
    "    \n",
    "    Team_Name = [i.text for i in soup.find_all('span',class_='u-hide-phablet')]\n",
    "            \n",
    "    # In Matches to extract the first match,points and rating we have different class than the rest of the body    \n",
    "        \n",
    "    Matches = []\n",
    "    Points = []\n",
    "    \n",
    "    match =  soup.find('td',class_='rankings-block__banner--matches')\n",
    "    Matches.append(match.text)\n",
    "    \n",
    "    point = soup.find('td',class_='rankings-block__banner--points')\n",
    "    Points.append(point.text)\n",
    "    \n",
    "    # Now we will extract the info from the body of the table    \n",
    "    # Here since the Matches and Points have same classes we create a condition where len(Matches)= 2\n",
    "    for j in soup.find_all('td',class_='table-body__cell u-center-text'):\n",
    "        if len(j.text)==2:\n",
    "            Matches.append(j.text)\n",
    "        else:\n",
    "            Points.append(j.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Ratings = []\n",
    "    \n",
    "    r =  soup.find('td',class_='rankings-block__banner--rating u-text-right')\n",
    "    Ratings.append(r.text.replace('\\n','').replace(' ',''))  \n",
    "    \n",
    "    \n",
    "    for rate in soup.find_all('td',class_ = 'table-body__cell u-text-right rating'):\n",
    "        Ratings.append(rate.text)\n",
    "    \n",
    "    \n",
    "    Team_Name = Team_Name[:10]\n",
    "    Matches = Matches[:10]\n",
    "    Ratings = Ratings[:10]\n",
    "    Points = Points[:10]\n",
    "    \n",
    "    data ={'Team Name':Team_Name,'Matches':Matches,'Points':Points,'Ratings':Ratings}\n",
    "    \n",
    "    df = pd.DataFrame(data,index=range(1,11))\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60b0da95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Team Name Matches Points Ratings\n",
      "1    New Zealand      23  2,670     116\n",
      "2        England      30  3,400     113\n",
      "3      Australia      32  3,572     112\n",
      "4          India      35  3,866     110\n",
      "5       Pakistan      22  2,354     107\n",
      "6   South Africa      24  2,392     100\n",
      "7     Bangladesh      30  2,753      92\n",
      "8      Sri Lanka      30  2,677      89\n",
      "9    Afghanistan      19  1,380      73\n",
      "10   West Indies      41  2,902      71\n"
     ]
    }
   ],
   "source": [
    "ODI_teams_mens(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253b7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fb0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf04af0",
   "metadata": {},
   "source": [
    "#### b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f68e293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d0f0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Bat_men(url):\n",
    "    \n",
    "    # Taking request from Website to perform scraping\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # We create a empty list for all attributes\n",
    "    \n",
    "    # In Matches to extract the name,team and rating we have different class than the rest of the body\n",
    "    Player_Name = []\n",
    "    #Extracting First name\n",
    "    name = soup.find('div',class_=\"rankings-block__banner--name\")\n",
    "    Player_Name.append(name.text)\n",
    "        \n",
    "    # Scraping names from rest of the body\n",
    "    \n",
    "    for i in soup.find_all('td',class_=\"table-body__cell name\"):\n",
    "        Player_Name.append(i.text.replace('\\n',''))\n",
    "    \n",
    "    Team = []\n",
    "\n",
    "    first = soup.find('div',class_='rankings-block__banner--nationality')\n",
    "    Team.append(first.text.split('\\n')[2])\n",
    "\n",
    "    for i in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "        Team.append(i.text)\n",
    "        \n",
    "        \n",
    "        \n",
    "    RATING = []\n",
    "\n",
    "    rate = soup.find('div',class_=\"rankings-block__banner--rating\")\n",
    "    RATING.append(rate.text)\n",
    "\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "        RATING.append(i.text)\n",
    "  \n",
    "     \n",
    "    Player_Name = Player_Name[:10]\n",
    "    Team = Team[:10]\n",
    "    RATING = RATING[:10]   \n",
    "\n",
    "    data = {'Player':Player_Name,'Team':Team,'Ratings':RATING}\n",
    "    \n",
    "    \n",
    "    if len(Player_Name)==len(Team)==len(RATING):\n",
    "        Batsmen_df = pd.DataFrame(data,index=range(1,11))\n",
    "    else:\n",
    "        print('please rerun the Whole code')\n",
    "    \n",
    "    \n",
    "    print(Batsmen_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0bb033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Player Team Ratings\n",
      "1              Babar Azam  PAK     890\n",
      "2             Imam-ul-Haq  PAK     779\n",
      "3   Rassie van der Dussen   SA     766\n",
      "4         Quinton de Kock   SA     759\n",
      "5            David Warner  AUS     747\n",
      "6             Steve Smith  AUS     719\n",
      "7          Jonny Bairstow  ENG     710\n",
      "8             Virat Kohli  IND     707\n",
      "9            Rohit Sharma  IND     704\n",
      "10        Kane Williamson   NZ     701\n"
     ]
    }
   ],
   "source": [
    "ODI_Bat_men(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51267f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876e85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71ac1984",
   "metadata": {},
   "source": [
    "#### c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d2eb1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f44e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Bowlers_men(url):\n",
    "    \n",
    "    \n",
    "    # Taking request from Website to perform scraping\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # We create a empty list for all attributes\n",
    "    \n",
    "    \n",
    "    Bowler_name = []\n",
    "     \n",
    "    #Extracting First name\n",
    "    \n",
    "    name = soup.find('div',class_='rankings-block__banner--name-large')\n",
    "    Bowler_name.append(name.text)\n",
    "    \n",
    "    # Scraping names from rest of the body\n",
    "    \n",
    "    for i in soup.find_all('td',class_='table-body__cell rankings-table__name name'):\n",
    "        Bowler_name.append(i.text.replace('\\n',''))\n",
    "    \n",
    "    \n",
    "    Bowler_team = []\n",
    "    \n",
    "    #Extracting First team name\n",
    "    \n",
    "    team = soup.find('div',class_='rankings-block__banner--nationality')\n",
    "    Bowler_team.append(team.text.replace('\\n','').replace(' ',''))\n",
    "    \n",
    "    # Scraping team names from rest of the body\n",
    "    \n",
    "    for i in soup.find_all('span',class_='table-body__logo-text'):\n",
    "        Bowler_team.append(i.text)\n",
    "    \n",
    "    \n",
    "    Bowler_Rating = []\n",
    "    \n",
    "    #Extracting First rating\n",
    "    \n",
    "    rates = soup.find('div',class_='rankings-block__banner--rating')\n",
    "    Bowler_Rating.append(rates.text)\n",
    "\n",
    "    # Scraping rating from rest of the body\n",
    "    \n",
    "    for i in soup.find_all('td',class_='table-body__cell rating'):\n",
    "        Bowler_Rating.append(i.text)\n",
    "    \n",
    "    Bowler_name = Bowler_name[:10]\n",
    "    Bowler_team = Bowler_team[:10]\n",
    "    Bowler_Rating = Bowler_Rating[:10]\n",
    "    \n",
    "    Bowler_data = {'Bowler Name':Bowler_name,'Team':Bowler_team,'Rating':Bowler_Rating}\n",
    "    \n",
    "    if len(Bowler_team)==len(Bowler_name)==len(Bowler_Rating):\n",
    "        \n",
    "        Bowler_df = pd.DataFrame(Bowler_data,index=range(1,11))\n",
    "        print(Bowler_df)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cad54e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Bowler Name Team Rating\n",
      "1         Trent Boult   NZ    760\n",
      "2      Josh Hazlewood  AUS    727\n",
      "3      Mitchell Starc  AUS    665\n",
      "4      Shaheen Afridi  PAK    661\n",
      "5          Matt Henry   NZ    656\n",
      "6          Adam Zampa  AUS    655\n",
      "7        Mehedi Hasan  BAN    655\n",
      "8    Mujeeb Ur Rahman  AFG    650\n",
      "9   Mustafizur Rahman  BAN    640\n",
      "10        Rashid Khan  AFG    635\n"
     ]
    }
   ],
   "source": [
    "ODI_Bowlers_men(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a39931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65a75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "886a3a6f",
   "metadata": {},
   "source": [
    "#### Q6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f92093bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71b20b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Women_ODI_Teams(url):\n",
    "    \n",
    "    # Taking request from Website to perform scraping\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # We create a empty list for all attributes\n",
    "    \n",
    "    Women_Team = []\n",
    "\n",
    "    for i in soup.find_all('span',class_='u-hide-phablet'):\n",
    "        Women_Team.append(i.text)\n",
    "    \n",
    "    \n",
    "    Women_Matches = []\n",
    "    Women_Team_Points = []\n",
    "\n",
    "    points = soup.find('td',class_='rankings-block__banner--points')\n",
    "    Women_Team_Points.append(points.text)\n",
    "\n",
    "    m = soup.find('td',class_='rankings-block__banner--matches')\n",
    "    Women_Matches.append(m.text)\n",
    "\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-center-text'):\n",
    "        if len(i.text)<=2:\n",
    "            Women_Matches.append(i.text)\n",
    "        else: \n",
    "            Women_Team_Points.append(i.text)\n",
    "    \n",
    "    \n",
    "    Women_Team_rating = []\n",
    "\n",
    "    r = soup.find('td',class_='rankings-block__banner--rating u-text-right')\n",
    "    Women_Team_rating.append(r.text.replace('\\n','').replace(' ',''))\n",
    "\n",
    "    for i in soup.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "        Women_Team_rating.append(i.text)\n",
    "       \n",
    "    Women_Team = Women_Team[:10]\n",
    "    Women_Matches = Women_Matches[:10]    \n",
    "    Women_Team_Points = Women_Team_Points[:10]    \n",
    "    Women_Team_rating = Women_Team_rating[:10]\n",
    "    \n",
    "    \n",
    "    data = {'Team(Women)':Women_Team,'Matches':Women_Matches,'Points':Women_Team_Points,'Rating':Women_Team_rating}\n",
    "    \n",
    "    \n",
    "    if len(Women_Team)==len(Women_Matches)==len(Women_Team_Points)==len(Women_Team_rating):\n",
    "        Women_team_df = pd.DataFrame(data,index=range(1,11))\n",
    "        print(Women_team_df)\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba57a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Team(Women) Matches Points Rating\n",
      "1      Australia      18  3,061    170\n",
      "2   South Africa      26  3,098    119\n",
      "3        England      25  2,904    116\n",
      "4          India      27  2,820    104\n",
      "5    New Zealand      24  2,425    101\n",
      "6    West Indies      24  2,334     97\n",
      "7     Bangladesh      12    932     78\n",
      "8       Thailand       8    572     72\n",
      "9       Pakistan      24  1,519     63\n",
      "10     Sri Lanka       8    353     44\n"
     ]
    }
   ],
   "source": [
    "Women_ODI_Teams(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915e618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935c5b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71b03563",
   "metadata": {},
   "source": [
    "#### b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bfe3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "111aac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Women_Batting(url):\n",
    "    # Taking request from Website to perform scraping\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # We create a empty list for all attributes\n",
    "    \n",
    "    Women_name = []\n",
    "\n",
    "\n",
    "    n = soup.find('div',class_='rankings-block__banner--name-large')\n",
    "    Women_name.append(n.text)\n",
    "\n",
    "    for i in soup.find_all('td',class_='table-body__cell rankings-table__name name'):\n",
    "        Women_name.append(i.text.replace('\\n',''))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    Women_Bat_Team = []\n",
    "\n",
    "    n = soup.find('div',class_='rankings-block__banner--nationality')\n",
    "    Women_Bat_Team.append(n.text.replace('\\n','').replace(' ',''))\n",
    "\n",
    "    for i in soup.find_all('span',class_='table-body__logo-text'):\n",
    "        Women_Bat_Team.append(i.text)\n",
    "    \n",
    "    \n",
    "    Women_Bat_Rating = []\n",
    "\n",
    "    r = soup.find('div',class_='rankings-block__banner--rating')\n",
    "    Women_Bat_Rating.append(r.text)\n",
    "\n",
    "    for i in soup.find_all('td',class_='table-body__cell rating'):\n",
    "        Women_Bat_Rating.append(i.text)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    Women_name = Women_name[:10]\n",
    "    Women_Bat_Team = Women_Bat_Team[:10]\n",
    "    Women_Bat_Rating = Women_Bat_Rating[:10]\n",
    "    \n",
    "    data = {'Name(Women)':Women_name,'Team':Women_Bat_Team,'Rating':Women_Bat_Rating}\n",
    "    \n",
    "    if len(Women_name)==len(Women_Bat_Team)==len(Women_Bat_Rating):\n",
    "        df = pd.DataFrame(data,index=range(1,11))\n",
    "        print(df)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c43705a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Name(Women) Team Rating\n",
      "1          Alyssa Healy  AUS    785\n",
      "2           Beth Mooney  AUS    749\n",
      "3       Laura Wolvaardt   SA    732\n",
      "4        Natalie Sciver  ENG    725\n",
      "5      Harmanpreet Kaur  IND    716\n",
      "6       Smriti Mandhana  IND    714\n",
      "7           Meg Lanning  AUS    710\n",
      "8        Rachael Haynes  AUS    701\n",
      "9     Amy Satterthwaite   NZ    661\n",
      "10  Chamari Athapaththu   SL    655\n"
     ]
    }
   ],
   "source": [
    "Women_Batting(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9729c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b45d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c53ed0b",
   "metadata": {},
   "source": [
    "#### c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e1a7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "680a5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Women_All_Rounder(url):\n",
    "    \n",
    "    # Taking request from Website to perform scraping\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # We create a empty list for all attributes\n",
    "    All_Name = []\n",
    "\n",
    "    name = soup.find('div',class_='rankings-block__banner--name-large')\n",
    "    All_Name.append(name.text)\n",
    "\n",
    "    for i in soup.find_all('td',class_='table-body__cell rankings-table__name name'):\n",
    "        All_Name.append(i.text.replace('\\n',''))\n",
    "    \n",
    "    \n",
    "    All_team = []\n",
    "\n",
    "    team = soup.find('div',class_='rankings-block__banner--nationality')\n",
    "    All_team.append(team.text.replace('\\n','').replace(' ',''))\n",
    "\n",
    "    for i in soup.find_all('span',class_='table-body__logo-text'):\n",
    "        All_team.append(i.text)\n",
    "    \n",
    "\n",
    "    All_Rating = []\n",
    "\n",
    "    rate = soup.find('div',class_='rankings-block__banner--rating')\n",
    "    All_Rating.append(rate.text)\n",
    "\n",
    "    for i in soup.find_all('td',class_='table-body__cell rating'):\n",
    "        All_Rating.append(i.text)\n",
    "    \n",
    "    All_Name = All_Name[:10]\n",
    "    All_team = All_team[:10]\n",
    "    All_Rating = All_Rating[:10]\n",
    "    \n",
    "    data = {'Name(Women)':All_Name,'Team':All_team,'Rating':All_Rating}\n",
    "    \n",
    "    if len(All_Name)==len(All_team)==len(All_Rating):\n",
    "        All_df = pd.DataFrame(data,index = range(1,11))\n",
    "        print(All_df)\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30829cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Name(Women) Team Rating\n",
      "1    Hayley Matthews   WI    380\n",
      "2       Ellyse Perry  AUS    374\n",
      "3     Natalie Sciver  ENG    357\n",
      "4        Amelia Kerr   NZ    356\n",
      "5     Marizanne Kapp   SA    349\n",
      "6      Deepti Sharma  IND    322\n",
      "7   Ashleigh Gardner  AUS    270\n",
      "8      Jess Jonassen  AUS    246\n",
      "9     Jhulan Goswami  IND    214\n",
      "10   Katherine Brunt  ENG    207\n"
     ]
    }
   ],
   "source": [
    "Women_All_Rounder(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c29545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de681a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "178c5ebb",
   "metadata": {},
   "source": [
    "#### Q7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "i) Headline ii) Time iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "183d8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cnbc.com/latest/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01c5cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def News_Headlines(url):\n",
    "    # Taking request from Website to perform scraping\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # We create a empty list for all attributes\n",
    "    \n",
    "    Headline = [news.text for news in soup.find_all('a',class_=\"Card-title\")]\n",
    "    Time = [time.text for time in soup.find_all('span',class_=\"Card-time\")  ]\n",
    "    Newslink = [link.a.get('href') for link in soup.find_all('div',class_='Card-titleContainer')]\n",
    "   \n",
    "    \n",
    "    print(Headline)\n",
    "    print(Newslink)\n",
    "    print(Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "085a2a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Why Silicon Valley is so hot on nuclear energy and what it means for the industry', 'Barclays downgrades Blackstone after firm limits real estate fund withdrawals', 'Buy this online real estate stock before the sector rebounds in 2024, UBS says', 'RBC downgrades DoorDash and slashes price target citing slowing order growth', 'Kremlin says Putin open to talks; Zelenskyy aide says up to 13,000 Ukrainian soldiers killed in war', 'U.S. Treasury yields mixed as traders await key U.S. jobs report', \"Elon Musk suspends Ye's Twitter account after swastika post\", 'OPEC+ to consider deeper oil output cuts ahead of Russia sanctions', 'European markets lower as investors await U.S. payrolls data', \"The U.S. wants the EU to be strict with China. But Europe can't afford it\", 'Rolls-Royce uses hydrogen produced with wind and tidal power to test jet engine', 'Singapore and New York are the most expensive cities to live in, EIU says', \"Hong Kong's Hang Seng still in bear market territory despite best month since 1998\", 'Tesla CEO Elon Musk kicks off first Semi truck deliveries', \"Citi names 6 global stocks that capture both 'defensive growth and value'\", \"BlackRock unit says it's time for a new portfolio playbook, and reveals how to position\", \"Cramer's lightning round: Seagate is not for this market\", \"Wrong Covid test results in China raise concerns 'the pandemic may never end'\", 'Jim Cramer says he likes these 3 restaurant stocks for a ‘normalizing’ economy', \"Asia-Pacific markets fall as investors seek clarity on China's Covid rule changes\", 'Powell’s inflation remarks are a ‘green light’ to stay in stocks, Cramer says', 'Stock futures fall slightly as investors await U.S. jobs data', \"Las Vegas Strip's biggest property owner in deal to take full ownership of two casinos\", 'Appeals court vacates order appointing Trump Mar-a-Lago search warrant watchdog', '4 ways to get free Wi-Fi on your next flight', \"Pro Picks: Watch all of Thursday's big stock calls on CNBC\", 'Stocks making the biggest moves after hours: Asana, Zscaler, Marvell and more', 'FCC authorizes SpaceX to begin deploying next-generation Starlink satellites', 'Fisker faces liquidity questions after short seller claims its cash is \"tied up\"', \"Ye's deal to buy conservative social media app Parler is called off\", 'CNN lays off hundreds of staffers after business review − read the memo', \"FDA pulls Covid treatment because it's not effective against dominant variants\", 'Mark Mahaney says this travel stock should weather a slowdown in demand', 'Ulta Beauty boosts outlook, as shoppers keep splurging on makeup', 'Senate approves bill enforcing railroad labor agreement before strike deadline']\n",
      "['https://www.cnbc.com/2022/12/02/why-silicon-valley-is-so-hot-on-nuclear-energy.html', 'https://www.cnbc.com/2022/12/02/russia-ukraine-live-updates.html', 'https://www.cnbc.com/2022/12/02/us-treasury-yields-ahead-of-key-us-jobs-report.html', 'https://www.cnbc.com/2022/12/02/elon-musk-suspends-yes-twitter-account-after-swastika-post.html', 'https://www.cnbc.com/2022/12/02/opec-meeting-oil-output-cuts-on-the-table-ahead-of-russia-sanctions.html', 'https://www.cnbc.com/2022/12/02/european-markets-open-to-close-as-investors-react-to-us-jobs-data.html', 'https://www.cnbc.com/2022/12/02/the-us-wants-the-eu-to-be-strict-with-china-but-europe-cant-afford-it.html', 'https://www.cnbc.com/2022/12/02/rolls-royce-uses-green-hydrogen-in-jet-engine-test.html', 'https://www.cnbc.com/2022/12/02/singapore-and-new-york-are-the-most-expensive-cities-to-live-in-eiu.html', 'https://www.cnbc.com/2022/12/02/hang-seng-still-in-bear-market-territory-despite-best-month-since-1998.html', 'https://www.cnbc.com/2022/12/01/tesla-ceo-elon-musk-kicks-off-semi-truck-deliveries.html', 'https://www.cnbc.com/2022/12/01/cramers-lightning-round-seagate-is-not-for-this-market.html', 'https://www.cnbc.com/2022/12/02/china-people-worry-wrong-covid-test-results-mean-pandemic-wont-end.html', 'https://www.cnbc.com/2022/12/01/jim-cramer-says-he-likes-these-3-restaurant-stocks.html', 'https://www.cnbc.com/2022/12/02/asia-pacific-markets-jobs.html', 'https://www.cnbc.com/2022/12/01/cramer-powells-inflation-remarks-a-green-light-to-stay-in-stocks.html', 'https://www.cnbc.com/2022/12/01/stock-market-futures-open-to-close-news.html', 'https://www.cnbc.com/2022/12/01/big-las-vegas-property-owner-to-take-full-ownership-of-two-casinos.html', 'https://www.cnbc.com/2022/12/01/appeals-court-vacates-order-appointing-trump-mar-a-lago-search-warrant-watchdog.html', 'https://www.cnbc.com/select/how-to-get-free-in-flight-wi-fi/', 'https://www.cnbc.com/2022/12/01/stocks-making-the-biggest-moves-after-hours-asana-zscaler-marvell-and-more.html', 'https://www.cnbc.com/2022/12/01/fcc-authorizes-spacex-gen2-starlink-up-to-7500-satellites.html', 'https://www.cnbc.com/2022/12/01/ev-maker-fisker-faces-liquidity-questions-after-short-seller-claims-its-cash-is-tied-up.html', 'https://www.cnbc.com/2022/12/01/ye-deal-to-buy-parler-called-off.html', 'https://www.cnbc.com/2022/12/01/cnn-lays-off-hundreds-of-staffers-read-the-memo.html', 'https://www.cnbc.com/2022/12/01/covid-fda-pulls-antibody-bebtelovimab-because-not-effective-against-omicron-bqpoint1.html', 'https://www.cnbc.com/2022/12/01/ulta-beauty-ulta-earnings-q3-2022.html', 'https://www.cnbc.com/2022/12/01/senate-passes-rail-labor-agreement-ahead-of-strike-sends-to-biden.html']\n",
      "['15 min ago', '34 min ago', 'an hour ago', '2 hours ago', '24 min ago', '3 hours ago', 'an hour ago', '4 hours ago', '43 min ago', '5 hours ago', '5 hours ago', '6 hours ago', '5 hours ago', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', '5 hours ago', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', '5 hours ago', 'Thu, Dec 1st 2022', 'Moments Ago', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022', 'Thu, Dec 1st 2022']\n"
     ]
    }
   ],
   "source": [
    "News_Headlines(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c083d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2028c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7356fc5b",
   "metadata": {},
   "source": [
    "#### Q.8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.Scrape below mentioned details :\n",
    "i) Paper Title ii) Authors iii) Published Date iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caca5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9324dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AI_Articles(url):\n",
    "    \n",
    "     # Taking request from Website to perform scraping\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # We create a empty list for all attributes\n",
    "    \n",
    "    Paper_Title = [i.text for i in soup.find_all('h2',class_='sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg') ]\n",
    "    Authors = [i.text for i in soup.find_all('span',class_='sc-1w3fpd7-0 dnCnAO')]\n",
    "    Published_Date = [i.text for i in soup.find_all('span',class_='sc-1thf9ly-2 dvggWt')]\n",
    "    Paper_URL = [i.get('href') for i in soup.find_all('a',class_=\"sc-5smygv-0 fIXTHm\") ]\n",
    "    \n",
    "    # Creating a DataFrame\n",
    "    \n",
    "    data = {'Paper Title':Paper_Title,'Authors':Authors,'Published Date':Published_Date,'Paper URL':Paper_URL}\n",
    "    df = pd.DataFrame(data,index=range(1,26))\n",
    "    \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a13ad9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Paper Title  \\\n",
      "1                                    Reward is enough   \n",
      "2                           Making sense of raw input   \n",
      "3   Law and logic: A review from an argumentation ...   \n",
      "4              Creativity and artificial intelligence   \n",
      "5   Artificial cognition for social human–robot in...   \n",
      "6   Explanation in artificial intelligence: Insigh...   \n",
      "7                       Making sense of sensory input   \n",
      "8   Conflict-based search for optimal multi-agent ...   \n",
      "9   Between MDPs and semi-MDPs: A framework for te...   \n",
      "10  The Hanabi challenge: A new frontier for AI re...   \n",
      "11  Evaluating XAI: A comparison of rule-based and...   \n",
      "12           Argumentation in artificial intelligence   \n",
      "13  Algorithms for computing strategies in two-pla...   \n",
      "14      Multiple object tracking: A literature review   \n",
      "15  Selection of relevant features and examples in...   \n",
      "16  A survey of inverse reinforcement learning: Ch...   \n",
      "17  Explaining individual predictions when feature...   \n",
      "18  A review of possible effects of cognitive bias...   \n",
      "19  Integrating social power into the decision-mak...   \n",
      "20  “That's (not) the output I expected!” On the r...   \n",
      "21  Explaining black-box classifiers using post-ho...   \n",
      "22  Algorithm runtime prediction: Methods & evalua...   \n",
      "23              Wrappers for feature subset selection   \n",
      "24  Commonsense visual sensemaking for autonomous ...   \n",
      "25         Quantum computation, quantum theory and AI   \n",
      "\n",
      "                                              Authors  Published Date  \\\n",
      "1   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
      "2           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
      "3                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
      "4                                 Boden, Margaret A.      August 1998   \n",
      "5     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
      "6                                        Miller, Tim    February 2019   \n",
      "7   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
      "8   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
      "9   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
      "10        Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
      "11  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
      "12               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
      "13       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
      "14             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
      "15                      Blum, Avrim L., Langley, Pat    December 1997   \n",
      "16                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
      "17      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
      "18  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
      "19    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
      "20                      Riveiro, Maria, Thill, Serge   September 2021   \n",
      "21  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
      "22  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
      "23                      Kohavi, Ron, John, George H.    December 1997   \n",
      "24  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
      "25                                   Ying, Mingsheng    February 2010   \n",
      "\n",
      "                                            Paper URL  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n",
      "24  https://www.sciencedirect.com/science/article/...  \n",
      "25  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "AI_Articles(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e25d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d5acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07ec1bfe",
   "metadata": {},
   "source": [
    "#### Q9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "i) Restaurant name ii) Cuisine iii) Location iv) Ratings v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c4a78be",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.dineout.co.in/delhi-restaurants/buffet-special'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2df124be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dineout(url):\n",
    "    \n",
    "     # Taking request from Website to perform scraping\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # We create a empty list for all attributes\n",
    "    \n",
    "    \n",
    "    Restaurant_name = [i.text for i in soup.find_all('a',class_='restnt-name ellipsis')]\n",
    "        \n",
    "    Cuisine = [i.text.split('|')[1] for i in soup.find_all('span',class_='double-line-ellipsis')] \n",
    "    \n",
    "    Location =[i.text for i in soup.find_all('div',class_='restnt-loc ellipsis')]\n",
    "        \n",
    "    Ratings = [i.text for i in soup.find_all('div',class_='restnt-rating rating-4')]\n",
    "   \n",
    "    Image_URL = [i.get('data-src') for i in soup.find_all('img',class_='no-img')]\n",
    "    \n",
    "    # Creating a DataFrame\n",
    "    \n",
    "    data = {'Restaurant Name':Restaurant_name,'Cuisine':Cuisine,'Location':Location,'Ratings':Ratings,'Image_URL':Image_URL}\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(data,index=range(1,10))\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3a08735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant Name                        Cuisine  \\\n",
      "1                   Castle Barbeque          Chinese, North Indian   \n",
      "2                   Jungle Jamboree   North Indian, Asian, Italian   \n",
      "3                        Cafe Knosh           Italian, Continental   \n",
      "4                   Castle Barbeque          Chinese, North Indian   \n",
      "5              The Barbeque Company          North Indian, Chinese   \n",
      "6                       India Grill          North Indian, Italian   \n",
      "7                    Delhi Barbeque                   North Indian   \n",
      "8  The Monarch - Bar Be Que Village                   North Indian   \n",
      "9                 Indian Grill Room          North Indian, Mughlai   \n",
      "\n",
      "                                            Location Ratings  \\\n",
      "1                     Connaught Place, Central Delhi     4.1   \n",
      "2             3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
      "3  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
      "4             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
      "5                 Gardens Galleria,Sector 38A, Noida       4   \n",
      "6               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
      "7     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.6   \n",
      "8  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
      "9   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
      "\n",
      "                                           Image_URL  \n",
      "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "9  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "Dineout(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2ffbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd15f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88755a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "810704d2",
   "metadata": {},
   "source": [
    "#### Q10) Write a python program to scrape the details of top publications from Google Scholar from\n",
    "i) Rank ii) Publication iii) h5-index iv) h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc07a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://scholar.google.com/citations?view_op=top_venues&hl=en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6496d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scholar_form(url):\n",
    "    # Taking request from Website to perform scraping\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    # We create a empty list for all attributes\n",
    "    \n",
    "    Publication = [i.text for i in soup.find_all('td',class_='gsc_mvt_t')]\n",
    "    \n",
    "    h5_index = [i.text for i in soup.find_all('a',class_='gs_ibl gsc_mp_anchor')]\n",
    "     \n",
    "    h5_median = [i.text for i in soup.find_all('span',class_='gs_ibl gsc_mp_anchor')]\n",
    "        \n",
    "    Rank = [i.text.replace('.','')  for i in soup.find_all('td',class_='gsc_mvt_p')]\n",
    "   \n",
    "    #Creating a DataFrame\n",
    "     \n",
    "    data = {'Rank':Rank,'Publication':Publication, 'h5-index':h5_index, 'h5-median':h5_median}\n",
    "    Publication_df = pd.DataFrame(data)\n",
    "    \n",
    "    Publication_df.set_index('Rank')\n",
    "    \n",
    "    \n",
    "    print(Publication_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "781f3a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rank                                        Publication h5-index h5-median\n",
      "0     1                                             Nature      444       667\n",
      "1     2                The New England Journal of Medicine      432       780\n",
      "2     3                                            Science      401       614\n",
      "3     4  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
      "4     5                                         The Lancet      354       635\n",
      "..  ...                                                ...      ...       ...\n",
      "95   96                       Journal of Business Research      145       233\n",
      "96   97                                   Molecular Cancer      145       209\n",
      "97   98                                            Sensors      145       201\n",
      "98   99                              Nature Climate Change      144       228\n",
      "99  100                    IEEE Internet of Things Journal      144       212\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "Scholar_form(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226edefc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
